{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Q 1.What is a parameter?\n",
        "**Ans** - In feature engineering, a parameter refers to a numerical or categorical value used to control or influence the transformation, selection, or generation of features from raw data. Parameters are often adjustable settings in feature transformation methods, and they help define how features are created or modified.\n",
        "\n",
        "**Examples of Parameters**\n",
        "1. Scaling Parameters - In Min-Max Scaling, the feature values are transformed using:\n",
        "        X' = (X-Xmin)/(Xmax-Xmin)\n",
        "Here, Xmin and Xmax are parameters.\n",
        "\n",
        "2. Polynomial Degree - In Polynomial Feature Engineering, the degree of the polynomial is a parameter that determines how many higher-order terms are created.\n",
        "3. Number of Bins - In Binning, the number of bins is a parameter that decides how the continuous data is split into discrete intervals.\n",
        "4. Encoding Parameters - In One-Hot Encoding, the presence or absence of categorical values is parameterized by binary variables.\n",
        "5. Window Size in Moving Averages - In Time Series Feature Engineering, the window size for calculating rolling statistics is a parameter."
      ],
      "metadata": {
        "id": "F5XLGlVSiUUj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 2. What is correlation? What does negative correlation mean?\n",
        "**Ans** - Correlation is a statistical measure that describes the relationship between two variables. It quantifies how one variable changes in relation to another. Correlation is commonly measured using Pearson's correlation coefficient (r), which ranges from -1 to 1:\n",
        "* r = 1 -> Perfect positive correlation.\n",
        "* r = 0 -> No correlation.\n",
        "* r = -1 -> Perfect negative correlation.\n",
        "\n",
        "Mathematically, Pearson's correlation coefficient is given by:\n",
        "\n",
        "    r = [∑(Xi-Xˉ)(Yi-Yˉ)] / [{∑(Xi-Xˉ)^2}^2 * {(∑(Yi-Yˉ)^2}^2]\n",
        "where:\n",
        "* Xi and Yi are individual values,\n",
        "* Xˉ and Yˉ are the means of the variables.\n",
        "\n",
        "**Negative Correlation means**\n",
        "\n",
        "A negative correlation means that as one variable increases, the other decreases.\n",
        "\n",
        "Examples of Negative Correlation:\n",
        "1. Temperature vs. Sweater Sales - As temperature increases, sweater sales decrease.\n",
        "2. Exercise vs. Body Fat Percentage - More exercise leads to lower body fat.\n",
        "3. Car Speed vs. Travel Time - Higher speed reduces travel time.\n",
        "\n",
        "If the correlation coefficient (r) is closer to -1, the negative correlation is stronger. If it is closer to 0, the relationship is weak."
      ],
      "metadata": {
        "id": "ksBl7oYdiWsk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 3. Define Machine Learning. What are the main components in Machine Learning?\n",
        "**Ans** - Machine Learning is a subset of artificial intelligence that enables computers to learn from data and make decisions or predictions without being explicitly programmed. Instead of following predefined rules, ML algorithms identify patterns in data and improve their performance over time.\n",
        "\n",
        "For example:\n",
        "* Task T: Predicting house prices\n",
        "* Experience E: Historical data of house prices\n",
        "* Performance P: Accuracy of price predictions\n",
        "\n",
        "**Main Components of Machine Learning**\n",
        "\n",
        "1. Data\n",
        "  * The foundation of ML; can be structured or unstructured.\n",
        "  * Needs preprocessing.\n",
        "2. Features\n",
        "  * Relevant attributes extracted from data to improve model performance.\n",
        "  * Example: In a house price model, features could be size, location, and number of bedrooms.\n",
        "3. Model\n",
        "  * The mathematical function that learns patterns from data.\n",
        "  * Examples: Linear Regression, Decision Trees, Neural Networks.\n",
        "4. Training Algorithm\n",
        "  * The method used to adjust model parameters based on training data.\n",
        "  * Example: Gradient Descent optimizes weights in neural networks.\n",
        "5. Loss Function\n",
        "  * Measures the difference between predicted and actual values.\n",
        "  * Example: Mean Squared Error for regression.\n",
        "6. Optimization Algorithm\n",
        "  * Adjusts the model to minimize the loss function.\n",
        "  * Example: Stochastic Gradient Descent.\n",
        "7. Evaluation Metrics\n",
        "  * Used to assess model performance on unseen data.\n",
        "  * Examples:\n",
        "    * Accuracy, Precision, Recall\n",
        "    * Mean Absolute Error, R² score\n",
        "8. Training & Testing Data Split\n",
        "  * Data is split into:\n",
        "    * Training set\n",
        "    * Test set\n",
        "    * Validation set\n",
        "9. Hyperparameters\n",
        "  * Configurations set before training.\n",
        "  * Example: Learning rate, number of hidden layers in a neural network.\n",
        "10. Deployment & Inference\n",
        "  * Once trained, the model is deployed to make real-world predictions.\n",
        "  * Example: A recommendation system suggesting products on Amazon."
      ],
      "metadata": {
        "id": "bKFrdxZaiYOW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 4. How does loss value help in determining whether the model is good or not?\n",
        "**Ans** - The loss value quantifies how well or poorly a machine learning model is performing. It represents the difference between the model's predictions and the actual ground truth values. A lower loss value generally indicates a better-performing model, while a higher loss suggests that the model is making significant errors.\n",
        "\n",
        "1. **Role of Loss in Model Evaluation**\n",
        "* Indicates Model Accuracy: A lower loss means the model is making better predictions.\n",
        "* Guides Optimization: The model updates its parameters to minimize loss.\n",
        "* Helps Identify Overfitting or Underfitting:\n",
        "  * High training loss & high validation loss - Underfitting\n",
        "  * Low training loss & high validation loss - Overfitting\n",
        "\n",
        "2. **Common Loss Functions**\n",
        "\n",
        "Loss functions differ based on the type of task:\n",
        "\n",
        "For Regression Problems:\n",
        "1. Mean Squared Error\n",
        "\n",
        "        MSE = 1/n * ∑(yi-y^i)2\n",
        "\n",
        "* Penalizes larger errors more than smaller ones.\n",
        "* Good for continuous output problems.\n",
        "2. Mean Absolute Error\n",
        "\n",
        "        MAE = 1/n * ∑|yi-y^i|\n",
        "* Measures absolute differences, making it less sensitive to outliers."
      ],
      "metadata": {
        "id": "4NPiWgbKiZjN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 5. What are continuous and categorical variables?\n",
        "**Ans** - In data science and statistics, variables are classified into different types based on their nature and the values they take. The two main types are continuous and categorical variables.\n",
        "\n",
        "1. **Continuous Variables**\n",
        "\n",
        "A continuous variable is a variable that can take any numerical value within a given range. These values are measured and can have decimal points.\n",
        "\n",
        "**Characteristics of Continuous Variables:**\n",
        "* Can take infinitely many values within a range\n",
        "* Can have decimal points\n",
        "* Often measured rather than counted\n",
        "\n",
        "**Examples:**\n",
        "* Height (e.g.,5.8ft, 175.3cm)\n",
        "* Weight (e.g.,65.5kg, 150.2lbs)\n",
        "* Temperature (e.g.,36.6°C, 98.4°F)\n",
        "* Time (e.g.,2.45sec, 5.67hrs)\n",
        "\n",
        "**Types of Continuous Variables:**\n",
        "* Interval Variable - Has no true zero (e.g., temperature in Celsius/Fahrenheit).\n",
        "* Ratio Variable - Has a true zero (e.g., weight, height, distance).\n",
        "\n",
        "2. **Categorical Variables**\n",
        "\n",
        "A categorical variable is a variable that represents categories or groups rather than numerical values. These values are counted, not measured.\n",
        "\n",
        "**Characteristics of Categorical Variables:**\n",
        "* Represent distinct groups or labels\n",
        "* Do not have numerical meaning\n",
        "* Can be nominal or ordinal\n",
        "\n",
        "**Examples:**\n",
        "* Gender (Male, Female, Other)\n",
        "* Blood Type (A, B, AB, O)\n",
        "* Color (Red, Blue, Green)\n",
        "* Education Level (High School, Bachelor's, Master's, Ph.D.)\n",
        "\n",
        "**Types of Categorical Variables:**\n",
        "* Nominal Variables - No meaningful order (e.g., eye color, car brand).\n",
        "* Ordinal Variables - Have a meaningful order but no fixed difference (e.g., education levels, satisfaction ratings: Low, Medium, High).\n",
        "\n",
        "**Differences Between Continuous and Categorical Variables**\n",
        "\n",
        "|Feature\t|Continuous Variable\t|Categorical Variable|\n",
        "|-|||\n",
        "|Definition\t|Measured values that can take any number in a range\t|Represents groups or categories|\n",
        "|Numerical?\t|Yes\t|No|\n",
        "|Decimals?\t|Yes\t|No|\n",
        "|Examples\t|Height, Weight, Temperature\t|Gender, Car Brand, Blood Type|\n",
        "|Subtypes\t|Interval, Ratio\t|Nominal, Ordinal|"
      ],
      "metadata": {
        "id": "54ikG6Q5ia_u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 6. How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "**Ans** - Categorical variables need to be converted into a numerical format before they can be used in machine learning models. There are several techniques to handle categorical data, and the best choice depends on the type of categorical variable and the algorithm used.\n",
        "\n",
        "**1. Encoding Techniques for Categorical Variables**\n",
        "\n",
        "(a) One-Hot Encoding\n",
        "* Converts each category into a separate binary column (0 or 1).\n",
        "* Best for nominal categorical variables.\n",
        "* Used in tree-based models and neural networks.\n",
        "\n",
        "**Example:**"
      ],
      "metadata": {
        "id": "CQvYkDMOicO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Color: ['Red', 'Blue', 'Green']"
      ],
      "metadata": {
        "id": "xR4jni9l2ut-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Pros: Works well with many machine learning models.\n",
        "* Cons: Can create too many columns for high-cardinality features.\n",
        "\n",
        "**Implementation in Python:**"
      ],
      "metadata": {
        "id": "9cD8NLjk3FlE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({'Color': ['Red', 'Blue', 'Green']})\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "encoded = encoder.fit_transform(df[['Color']])\n",
        "print(encoded)"
      ],
      "metadata": {
        "id": "RZmTwvur3WLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) Label Encoding\n",
        "* Assigns a unique integer to each category.\n",
        "* Best for ordinal categorical variables.\n",
        "* Used in models that can handle ordinal relationships.\n",
        "\n",
        "Example:"
      ],
      "metadata": {
        "id": "UHtXJUXs3eUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Education: ['High School', 'Bachelor', 'Master', 'PhD']"
      ],
      "metadata": {
        "id": "HUDlnuHZ3u9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Pros: Simple and memory-efficient.\n",
        "* Cons: Can mislead models that assume numerical relationships where none exist.\n",
        "\n",
        "Implementation in Python:"
      ],
      "metadata": {
        "id": "6EAw5tSI4Cd4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "df = pd.DataFrame({'Education': ['High School', 'Bachelor', 'Master', 'PhD']})\n",
        "encoder = LabelEncoder()\n",
        "df['Education_encoded'] = encoder.fit_transform(df['Education'])\n",
        "print(df)"
      ],
      "metadata": {
        "id": "gX22kVPp4Nz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(c) Ordinal Encoding\n",
        "* Similar to Label Encoding but explicitly defines an order.\n",
        "* Used when categories have a meaningful ranking.\n",
        "\n",
        "Example:"
      ],
      "metadata": {
        "id": "Oe3WsS8k4ahz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Size: ['Small', 'Medium', 'Large']"
      ],
      "metadata": {
        "id": "CyRLpEFC4k7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementation in Python:"
      ],
      "metadata": {
        "id": "N4j_6QHN43mt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "df = pd.DataFrame({'Size': ['Small', 'Medium', 'Large']})\n",
        "encoder = OrdinalEncoder(categories=[['Small', 'Medium', 'Large']])\n",
        "df['Size_encoded'] = encoder.fit_transform(df[['Size']])\n",
        "print(df)"
      ],
      "metadata": {
        "id": "jZj0wcDO5Bsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(d) Target Encoding\n",
        "* Replaces categories with the mean of the target variable.\n",
        "* Works well for high-cardinality categorical variables.\n",
        "* Commonly used in regression problems.\n",
        "\n",
        "Example:\n",
        "For a dataset predicting house prices:"
      ],
      "metadata": {
        "id": "fpvKj-vG5F6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Neighborhood: ['A', 'B', 'C']\n",
        "Average Price: [200k, 250k, 300k]"
      ],
      "metadata": {
        "id": "Ra51Z-f_5Rur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Pros: Reduces dimensionality compared to One-Hot Encoding.\n",
        "* Cons: Can cause data leakage if applied incorrectly.\n",
        "\n",
        "Implementation in Python:"
      ],
      "metadata": {
        "id": "qHs0vdSB5h0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({'Neighborhood': ['A', 'B', 'A', 'C'], 'Price': [200, 250, 220, 300]})\n",
        "df['Neighborhood_encoded'] = df.groupby('Neighborhood')['Price'].transform('mean')\n",
        "print(df)"
      ],
      "metadata": {
        "id": "e5LOkAcW5uzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(e) Frequency Encoding\n",
        "* Replaces categories with their occurrence count in the dataset.\n",
        "* Useful when some categories appear much more frequently than others.\n",
        "\n",
        "Example:"
      ],
      "metadata": {
        "id": "nRVYIO4j5yjz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "City: ['New York', 'Los Angeles', 'Chicago', 'New York', 'Chicago']"
      ],
      "metadata": {
        "id": "U6WWHK7J5-L2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Pros: Keeps useful information about category importance.\n",
        "* Cons: May not always capture meaningful patterns.\n",
        "\n",
        "Implementation in Python:"
      ],
      "metadata": {
        "id": "0wnzw5wn6KiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({'City': ['NY', 'LA', 'Chicago', 'NY', 'Chicago']})\n",
        "df['City_encoded'] = df['City'].map(df['City'].value_counts())\n",
        "print(df)"
      ],
      "metadata": {
        "id": "zjuBGW4M6Rf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Handling High-Cardinality Categorical Variables**\n",
        "\n",
        "When a categorical feature has many unique values, encoding it efficiently is important:\n",
        "* Target Encoding\n",
        "* Frequency Encoding\n",
        "* Feature Hashing - Converts categories into a fixed number of hash-based numerical columns.\n",
        "\n",
        "Example using Feature Hashing:"
      ],
      "metadata": {
        "id": "t4qrXZLb6VME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction import FeatureHasher\n",
        "\n",
        "df = pd.DataFrame({'Product_ID': ['A123', 'B456', 'C789', 'A123']})\n",
        "hasher = FeatureHasher(n_features=3, input_type='string')\n",
        "hashed_features = hasher.transform(df['Product_ID'])\n",
        "print(hashed_features.toarray())"
      ],
      "metadata": {
        "id": "RnJfhkrR6rrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Choosing the Right Encoding Technique**\n",
        "\n",
        "|Encoding Type\t|Use Case|\n",
        "|-||\n",
        "|One-Hot Encoding (OHE)\t|Small number of categories (nominal data)|\n",
        "|Label Encoding\t|Ordinal data (e.g., Education Level)|\n",
        "|Ordinal Encoding\t|When categories have a specific order|\n",
        "|Target Encoding\t|High-cardinality categorical data (regression problems)|\n",
        "|Frequency Encoding\t|When category frequency matters|\n",
        "|Feature Hashing\t|Large-scale categorical features|"
      ],
      "metadata": {
        "id": "-HZXwhi26vce"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 7. What do you mean by training and testing a dataset?\n",
        "**Ans** - In machine learning, data is typically split into two main sets:\n",
        "1. Training Dataset - Used to train the model.\n",
        "2. Testing Dataset - Used to evaluate the model's performance on unseen data.\n",
        "\n",
        "**1. Training Dataset**\n",
        "* The training dataset is the portion of data that the model learns from.\n",
        "* It contains input features and corresponding target labels.\n",
        "* The model identifies patterns and adjusts parameters based on this data.\n",
        "* The goal is to minimize loss during training.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "A dataset predicting house prices based on features like size and location.\n",
        "\n",
        "|Size (sq ft)\t|Location\t|Price ($1000s)|\n",
        "|-|||\n",
        "|1500\t|Urban\t|300|\n",
        "|1800\t|Suburban\t|250|\n",
        "|1200\t|Rural\t|200|\n",
        "\n",
        "* The model learns from this data and tries to find the relationship between size, location, and price.\n",
        "\n",
        "**2. Testing Dataset**\n",
        "* The testing dataset is used to evaluate how well the trained model generalizes to new, unseen data.\n",
        "* It helps detect overfitting.\n",
        "* The model does not learn from this dataset—only performance is measured.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "|Size (sq ft)\t|Location\t|Price ($1000s)|\n",
        "|-|||\n",
        "|1600\t|Urban\t|???|\n",
        "|1400\t|Suburban\t|???|\n",
        "\n",
        "* The model predicts the price for the test data and compares it with actual values."
      ],
      "metadata": {
        "id": "eNLB08rDi_op"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.DataFrame({\n",
        "    'Size': [1500, 1800, 1200, 1600, 1400],\n",
        "    'Location': ['Urban', 'Suburban', 'Rural', 'Urban', 'Suburban'],\n",
        "    'Price': [300, 250, 200, 275, 230]\n",
        "})\n",
        "\n",
        "train_set, test_set = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Training Set:\\n\", train_set)\n",
        "print(\"\\nTesting Set:\\n\", test_set)"
      ],
      "metadata": {
        "id": "VV5VOpBT9mSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 8. What is sklearn.preprocessing?\n",
        "**Ans** - sklearn.preprocessing is a module in Scikit-Learn that provides tools for transforming raw data into a format suitable for machine learning models. It includes functions for scaling, encoding, imputing missing values, and normalizing data.\n",
        "\n",
        "**1. Use of sklearn.preprocessing?**\n",
        "* Improves model performance by ensuring features are properly scaled.\n",
        "* Handles categorical variables through encoding.\n",
        "* Deals with missing data by imputing values.\n",
        "* Standardizes features to ensure models converge faster.\n",
        "\n",
        "**2. Common sklearn.preprocessing Techniques**\n",
        "\n",
        "(a) Standardization\n",
        "* Scales data to have mean = 0 and standard deviation = 1.\n",
        "* Helps models like Logistic Regression, SVMs, and Neural Networks perform better."
      ],
      "metadata": {
        "id": "XGPogYyBjCTR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) Min-Max Scaling\n",
        "* Scales data between 0 and 1.\n",
        "* Used in algorithms like KNN and Neural Networks."
      ],
      "metadata": {
        "id": "0V_JMAio-jx7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(c) Encoding Categorical Variables\n",
        "\n",
        "One-Hot Encoding\n",
        "* Converts categories into binary columns."
      ],
      "metadata": {
        "id": "CHrlSaID-z6Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Label Encoding\n",
        "* Assigns unique integer values to categories."
      ],
      "metadata": {
        "id": "cQ27v6yj_CaW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(d) Imputation\n",
        "* Fills missing values using mean, median, or most frequent values."
      ],
      "metadata": {
        "id": "no5fzjAL_PtR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(e) Binarization\n",
        "* Converts numerical values into 0s and 1s based on a threshold."
      ],
      "metadata": {
        "id": "cM8QQie3_ZKu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Choosing the Right Preprocessing Method**\n",
        "\n",
        "|Task\t|Method|\n",
        "|-||\n",
        "|Standardizing data\t|StandardScaler|\n",
        "|Normalizing data\t|MinMaxScaler|\n",
        "|Encoding categorical data\t|OneHotEncoder, LabelEncoder|\n",
        "|Handling missing values\t|SimpleImputer|\n",
        "|Binarizing data\t|Binarizer|"
      ],
      "metadata": {
        "id": "4tmf8djZ_mtO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 9. What is a Test set?\n",
        "**Ans** - A test set is a portion of a dataset that is not used for training the model but is instead used to evaluate the model's performance on unseen data. It helps assess how well the model generalizes to new, real-world data.\n",
        "\n",
        "**1. Purpose of a Test Set**\n",
        "* Evaluates model performance on unseen data.\n",
        "* Detects overfitting.\n",
        "* Compares different models before selecting the best one.\n",
        "* Ensures unbiased performance measurement before deployment."
      ],
      "metadata": {
        "id": "_6RAnDf_jEuv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 10. How do we split data for model fitting (training and testing) in Python? How do you approach a Machine Learning problem?\n",
        "**Ans** - **1. Split Data for Model Training and Testing in Python**\n",
        "\n",
        "In machine learning, we split data into two sets:\n",
        "* Training Set - Used to train the model (usually 70-80%).\n",
        "* Test Set - Used to evaluate model performance (20-30%).\n",
        "\n",
        "Using 'train_test_split' from sklearn"
      ],
      "metadata": {
        "id": "2Go-CfIujGLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.DataFrame({\n",
        "    'Feature1': [10, 20, 30, 40, 50, 60, 70, 80],\n",
        "    'Feature2': [5, 15, 25, 35, 45, 55, 65, 75],\n",
        "    'Label': [1, 0, 1, 0, 1, 0, 1, 0]\n",
        "})\n",
        "\n",
        "train_set, test_set = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Training Set:\\n\", train_set)\n",
        "print(\"\\nTesting Set:\\n\", test_set)"
      ],
      "metadata": {
        "id": "S8uZ2ePAB_xP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* test_size = 0.2 - Allocates 20% of data for testing.\n",
        "* random_state = 42 - Ensures reproducibility.\n",
        "\n",
        "Splitting Data into Training, Validation, and Test Sets\n",
        "\n",
        "If we need a validation set:"
      ],
      "metadata": {
        "id": "Pg3POtvjCJa2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set, test_set = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "train_set, val_set = train_test_split(train_set, test_size=0.25, random_state=42)\n",
        "\n",
        "print(\"Training Set:\\n\", train_set)\n",
        "print(\"\\nValidation Set:\\n\", val_set)\n",
        "print(\"\\nTesting Set:\\n\", test_set)"
      ],
      "metadata": {
        "id": "BuCv779NCWw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* This results in 60% train, 20% validation, 20% test.\n",
        "\n",
        "**2. Approach to Machine Learning Problem**\n",
        "\n",
        "To build an effective ML model, we follow these steps:\n",
        "\n",
        "**Step 1: Define the Problem**\n",
        "* Understand the problem statement.\n",
        "* Identify the type of problem:\n",
        "  * Regression\n",
        "  * Classification\n",
        "  * Clustering\n",
        "\n",
        "Example: Predict house prices based on size and location.\n",
        "\n",
        "**Step 2: Collect & Explore Data**\n",
        "* Load the dataset.\n",
        "* Check for missing values & outliers.\n",
        "* Visualize data.\n",
        "\n",
        "Example in Python:"
      ],
      "metadata": {
        "id": "yBrZ9SxfCbF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = pd.read_csv(\"house_prices.csv\")\n",
        "\n",
        "print(df.isnull().sum())\n",
        "\n",
        "df.hist(figsize=(8, 6))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9t36teHLC5H1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Preprocess & Clean Data**\n",
        "* Handle missing values.\n",
        "* Convert categorical variables.\n",
        "* Scale numerical features.\n",
        "\n",
        "Example in Python:"
      ],
      "metadata": {
        "id": "XxlAOUbjC-B1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "import numpy as np\n",
        "\n",
        "imputer = SimpleImputer(strategy=\"mean\")\n",
        "df['Size'] = imputer.fit_transform(df[['Size']])\n",
        "\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "encoded_location = encoder.fit_transform(df[['Location']])\n",
        "\n",
        "scaler = StandardScaler()\n",
        "df[['Size', 'Price']] = scaler.fit_transform(df[['Size', 'Price']])"
      ],
      "metadata": {
        "id": "w73sGrREDLJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Split Data (Train-Test)**\n",
        "* Use train_test_split() to divide data.\n",
        "* Optionally, create a validation set for hyperparameter tuning."
      ],
      "metadata": {
        "id": "HdswHRh6DO46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df.drop(columns=['Price'])\n",
        "y = df['Price']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "LiMTqNnzDans"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5: Choose & Train a Model**\n",
        "* Select an appropriate machine learning algorithm.\n",
        "* Train the model using fit().\n",
        "\n",
        "Example using Linear Regression:"
      ],
      "metadata": {
        "id": "RES5W7LaDePK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "MgLS-k-IDobi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 6: Evaluate the Model**\n",
        "* Use test data to check model performance.\n",
        "* Metrics depend on the problem type:\n",
        "  * Regression: RMSE, MAE, R²\n",
        "  * Classification: Accuracy, Precision, Recall, F1-score\n",
        "\n",
        "Example for Regression:"
      ],
      "metadata": {
        "id": "Xmny4YsjDrfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)"
      ],
      "metadata": {
        "id": "0RSzbnujD5Vx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 7: Tune Hyperparameters (Optional)**\n",
        "* Use GridSearchCV or RandomizedSearchCV to find the best hyperparameters."
      ],
      "metadata": {
        "id": "GJu6iDIOD86h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {'fit_intercept': [True, False]}\n",
        "grid_search = GridSearchCV(LinearRegression(), param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "print(grid_search.best_params_)"
      ],
      "metadata": {
        "id": "W_gdCsALEHDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 8: Deploy the Model**\n",
        "* Save the trained model using joblib or pickle.\n",
        "* Deploy it using Flask, FastAPI, or cloud services.\n",
        "\n",
        "Saving Model:"
      ],
      "metadata": {
        "id": "d_ef7QmOEKHW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "joblib.dump(model, 'house_price_model.pkl')"
      ],
      "metadata": {
        "id": "tKeDOM6vETAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary of Machine Learning Workflow**\n",
        "\n",
        "|Step\t|Action|\n",
        "|-||\n",
        "|Define Problem\t|Identify task|\n",
        "|Collect Data\t|Load dataset, check for missing values|\n",
        "|Preprocess Data\t|Encode categorical, scale numerical data|\n",
        "|Split Data\t|Train-test split (80-20 or 70-30)|\n",
        "|Train Model\t|Select ML algorithm, fit model|\n",
        "|Evaluate Model\t|Use metrics like RMSE, accuracy|\n",
        "|Hyperparameter Tuning\t|Optimize parameters for better performance|\n",
        "|Deploy Model\t|Save model and deploy|"
      ],
      "metadata": {
        "id": "XEfh0NhfEXa8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 11. Why do we have to perform EDA before fitting a model to the data?\n",
        "**Ans** - Exploratory Data Analysis is a crucial step in machine learning where we analyze, visualize, and preprocess data before fitting a model. Skipping EDA can lead to poor model performance, biased predictions, or incorrect conclusions.\n",
        "\n",
        "**1. Understand Data Structure & Quality**\n",
        "* Check data types.\n",
        "* Identify missing values, duplicates, and inconsistencies.\n",
        "* Detect outliers that may affect model training.\n",
        "\n",
        "**Example: Checking data types & missing values**"
      ],
      "metadata": {
        "id": "gvuw0VhgjH5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"house_prices.csv\")\n",
        "\n",
        "print(df.dtypes)\n",
        "\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "HNfbfo_fFa-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Detect & Handle Missing Values**\n",
        "* Missing values can bias model predictions.\n",
        "* Solutions:\n",
        "  * Drop rows/columns with too many missing values.\n",
        "  * Impute missing values using mean, median, mode, or predictive models.\n",
        "\n",
        "Example: Imputing missing values"
      ],
      "metadata": {
        "id": "8heOBO1uFgbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "imputer = SimpleImputer(strategy=\"mean\")\n",
        "df['Size'] = imputer.fit_transform(df[['Size']])"
      ],
      "metadata": {
        "id": "arzhZE1lFxXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Identify Outliers & Handle Them**\n",
        "* Outliers can distort model learning.\n",
        "* Use boxplots, histograms, or z-scores to detect them.\n",
        "\n",
        "Example: Detecting Outliers Using Boxplot"
      ],
      "metadata": {
        "id": "aTAJFrQoF1qY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.boxplot(x=df['Price'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZKqZjoYOGCE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Solutions:**\n",
        "* Remove extreme outliers.\n",
        "* Apply log transformation or clipping to reduce impact.\n",
        "\n",
        "**4. Detect Feature Relationships & Multicollinearity**\n",
        "* Check correlation between features.\n",
        "* Multicollinearity can confuse models.\n",
        "\n",
        "Example: Correlation Matrix"
      ],
      "metadata": {
        "id": "mMxdMaGHGGgF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2CnciQn0IcGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Solution for Multicollinearity:\n",
        "  * Remove redundant features.\n",
        "  * Use Principal Component Analysis.\n",
        "\n",
        "**5. Choose the Right Feature Engineering Approach**\n",
        "* Categorical features need encoding.\n",
        "* Numerical features need scaling.\n",
        "\n",
        "Example: Encoding Categorical Variables"
      ],
      "metadata": {
        "id": "oPke1mCyIfeK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "encoded_features = encoder.fit_transform(df[['Location']])"
      ],
      "metadata": {
        "id": "ll2n4KCvJAuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example: Scaling Numerical Features"
      ],
      "metadata": {
        "id": "V1Z8VrNAJEPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "df[['Size', 'Price']] = scaler.fit_transform(df[['Size', 'Price']])"
      ],
      "metadata": {
        "id": "wpf3yuPJJJ_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Avoid Data Leakage & Bias**\n",
        "* Ensure that no data from the test set leaks into training.\n",
        "* Balance classes in imbalanced datasets (e.g., fraud detection).\n",
        "\n",
        "Example: Handling Imbalanced Data Using SMOTE"
      ],
      "metadata": {
        "id": "EjQDo8cEJM-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "smote = SMOTE()\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)"
      ],
      "metadata": {
        "id": "aO6djmW8JXS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reasons for performing EDA**\n",
        "\n",
        "|EDA Task\t|Why It Matters?|\n",
        "|-||\n",
        "|Check Missing Values\t|Prevents biased models|\n",
        "|Identify Outliers\t|Avoids extreme values affecting learning|\n",
        "|Feature Correlation\t|Removes redundant variables|\n",
        "|Encode Categorical Data\t|Ensures models can process categorical variables|\n",
        "|Normalize Features\t|Helps algorithms converge faster|\n",
        "|Avoid Data Leakage\t|Prevents unfair performance estimation|"
      ],
      "metadata": {
        "id": "KL3VQuANJaLJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 12. What is correlation?\n",
        "**Ans** - Correlation is a statistical measure that describes the strength and direction of the relationship between two variables. It tells us how one variable changes in relation to another.\n",
        "* Positive Correlation: If one variable increases, the other also increases.\n",
        "* Negative Correlation: If one variable increases, the other decreases.\n",
        "* No Correlation: No relationship between the variables.\n",
        "\n",
        "**1. Pearson's Correlation Coefficient (r)**\n",
        "The most common measure of correlation is Pearson’s Correlation Coefficient (r), which ranges from -1 to 1:\n",
        "\n",
        "    r = [∑(Xi-Xˉ)(Yi-Yˉ)] / [{∑(Xi-Xˉ)^2}{∑(Yi-Yˉ)^2}]\n",
        "\n",
        "Interpretation of r:\n",
        "\n",
        "|Value of r\t|Interpretation|\n",
        "|-||\n",
        "|r = +1\t|Perfect positive correlation|\n",
        "|r = 0.7 to 0.99\t|Strong positive correlation|\n",
        "|r = 0.3 to 0.69\t|Moderate positive correlation|\n",
        "|r = 0\t|No correlation|\n",
        "|r = -0.3 to -0.69\t|Moderate negative correlation|\n",
        "|r = -0.7 to -0.99\t|Strong negative correlation|\n",
        "|r = -1\t|Perfect negative correlation|\n",
        "\n",
        "**2. Example of Correlation**\n",
        "\n",
        "Positive Correlation Example\n",
        "* Height vs. Weight\n",
        "  * Taller people tend to weigh more.\n",
        "  * r ≈ +0.8\n",
        "\n",
        "Negative Correlation Example\n",
        "* Temperature vs. Hot Coffee Sales\n",
        "  * When the temperature increases, coffee sales decrease.\n",
        "  * r ≈ -0.7\n",
        "\n",
        "No Correlation Example\n",
        "* Shoe Size vs. Exam Scores\n",
        "  * Shoe size has no effect on exam scores.\n",
        "  * r ≈ 0\n",
        "\n",
        "**3. Visualizing Correlation**\n",
        "\n",
        "Example in Python"
      ],
      "metadata": {
        "id": "Ritdg9qJjJAE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = pd.DataFrame({'Height': [150, 160, 170, 180, 190], 'Weight': [50, 60, 70, 80, 90]})\n",
        "\n",
        "correlation = df.corr()\n",
        "print(correlation)\n",
        "\n",
        "sns.heatmap(correlation, annot=True, cmap='coolwarm')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HC5Q8WaIWkiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 13. What does negative correlation mean?\n",
        "**Ans** - A negative correlation means that as one variable increases, the other decreases.\n",
        "\n",
        "Mathematically, the Pearson correlation coefficient (r) for negative correlation is between -1 and 0:\n",
        "* r = -1 -> Perfect negative correlation.\n",
        "* r = -0.7 to -0.99 -> Strong negative correlation.\n",
        "* r = -0.3 to -0.69 -> Moderate negative correlation.\n",
        "* r = 0 -> No correlation.\n",
        "\n",
        "**Examples of Negative Correlation**\n",
        "* Temperature vs. Hot Coffee Sales - As temperature increases, coffee sales decrease.\n",
        "* Exercise vs. Body Fat Percentage - More exercise leads to lower body fat.\n",
        "* Speed vs. Travel Time - As car speed increases, travel time decreases.\n",
        "\n",
        "**Visualizing Negative Correlation**\n",
        "\n",
        "Example in Python"
      ],
      "metadata": {
        "id": "FjFL8W-qjPof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = np.array([1, 2, 3, 4, 5])\n",
        "y = np.array([10, 8, 6, 4, 2])\n",
        "\n",
        "plt.scatter(x, y, color='red')\n",
        "plt.xlabel(\"Exercise Hours\")\n",
        "plt.ylabel(\"Body Fat Percentage\")\n",
        "plt.title(\"Negative Correlation Example\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aK8F0w6eYFwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 14. How can you find correlation between variables in Python?\n",
        "**Ans** - In Python, we can compute correlation using Pandas, NumPy, and Seaborn to analyze relationships between numerical variables.\n",
        "\n",
        "**1. Using corr() in Pandas**\n",
        "\n",
        "Pandas provides the .corr() method to compute correlation between numerical columns.\n",
        "\n",
        "Example: Compute Correlation Matrix"
      ],
      "metadata": {
        "id": "7Z0ePaUqjRKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = {'Height': [150, 160, 170, 180, 190],\n",
        "        'Weight': [50, 60, 70, 80, 90],\n",
        "        'Age': [20, 25, 30, 35, 40]}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "correlation_matrix = df.corr()\n",
        "print(correlation_matrix)"
      ],
      "metadata": {
        "id": "LA2vt5m08c08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* By default, .corr() calculates Pearson’s correlation.\n",
        "* The closer the value is to +1 or -1, the stronger the correlation.\n",
        "\n",
        "**2. Using NumPy’s corrcoef()**\n",
        "\n",
        "NumPy's corrcoef() calculates correlation between two variables."
      ],
      "metadata": {
        "id": "uQ-GCm6l8gn6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "height = np.array([150, 160, 170, 180, 190])\n",
        "weight = np.array([50, 60, 70, 80, 90])\n",
        "\n",
        "correlation = np.corrcoef(height, weight)\n",
        "print(correlation)"
      ],
      "metadata": {
        "id": "WWu5esef827W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The result is a correlation matrix where correlation[0,1] gives the correlation between the two variables.\n",
        "\n",
        "**3. Visualizing Correlation with Seaborn & Matplotlib**\n",
        "\n",
        "A heatmap helps visualize correlation between multiple variables."
      ],
      "metadata": {
        "id": "Gj3fExZc87Im"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "correlation_matrix = df.corr()\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0sxPVyWAOyOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Red/Blue shades indicate strong positive/negative correlation.\n",
        "* Lighter shades suggest weak or no correlation.\n",
        "\n",
        "**4. Compute Spearman & Kendall Correlation**\n",
        "\n",
        "Besides Pearson's correlation, you can also compute:\n",
        "* Spearman's Rank Correlation\n",
        "* Kendall's Tau Correlation"
      ],
      "metadata": {
        "id": "HrW258KfO2Zh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.corr(method='spearman')\n",
        "df.corr(method='kendall')"
      ],
      "metadata": {
        "id": "mfHlLu9oPFt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary of Methods**\n",
        "\n",
        "|Method\t|Use Case|\n",
        "|-||\n",
        "|.corr() (Pandas)\t|Compute correlation matrix for all numerical variables|\n",
        "|corrcoef() (NumPy)\t|Compute correlation between two specific variables|\n",
        "|heatmap() (Seaborn)\t|Visualize correlations in a heatmap|\n",
        "|method='spearman'\t|Use for ordinal/ranked data|\n",
        "|method='kendall'\t|Use for small datasets with ranked variables|"
      ],
      "metadata": {
        "id": "Z3VWN8vlPIl6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 15. What is causation? Explain difference between correlation and causation with an example.\n",
        "**Ans** - Causation means that one event directly affects another. If X causes Y, then changing X will result in a change in Y.\n",
        "\n",
        "For example:\n",
        "* More exercise - Weight loss.\n",
        "* Smoking - Lung cancer.\n",
        "\n",
        "**Difference Between Correlation and Causation**\n",
        "* Correlation means two variables are related, but one does NOT necessarily cause the other.\n",
        "* Causation means one variable directly affects the other.\n",
        "\n",
        "**Example: Ice Cream Sales & Drowning Cases**\n",
        "* Observation: Ice cream sales and drowning cases are positively correlated.\n",
        "* Does this mean eating ice cream causes drowning? No!\n",
        "* Real Cause: Hot Weather increases both ice cream sales and swimming activity, leading to more drowning cases.\n",
        "* This is correlation, NOT causation.\n",
        "\n",
        "**Key Differences**\n",
        "\n",
        "|Feature\t|Correlation\t|Causation|\n",
        "|-|||\n",
        "|Definition\t|A statistical relationship between two variables.\t|One variable directly causes a change in another.|\n",
        "|Directionality\t|No clear direction.\t|X directly affects Y.|\n",
        "|Example\t|Higher ice cream sales & drowning cases.\t|More smoking causes lung cancer.|\n",
        "|Proves Cause?\t|No\t|Yes|"
      ],
      "metadata": {
        "id": "6dBZTYgojS-D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 16. What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "**Ans** - An optimizer is an algorithm that adjusts the parameters of a machine learning model to minimize the loss function and improve model performance. It helps the model learn patterns from data by updating weights in the right direction during training.\n",
        "\n",
        "**Types of Optimizers in Machine Learning**\n",
        "\n",
        "Optimizers can be broadly classified into two categories:\n",
        "1. Gradient Descent-Based Optimizers\n",
        "2. Non-Gradient-Based Optimizers\n",
        "\n",
        "**1. Gradient Descent-Based Optimizers**\n",
        "\n",
        "Gradient Descent is an optimization technique that updates model parameters by computing the gradient of the loss function and adjusting weights accordingly.\n",
        "\n",
        "**(a) Batch Gradient Descent**\n",
        "* Uses all training data to compute the gradient before updating weights.\n",
        "* Converges smoothly but is slow for large datasets.\n",
        "\n",
        "Formula for Weight Update:\n",
        "\n",
        "    W = W - α*(dL/dW)\n",
        "where:\n",
        "* W = weights\n",
        "* α = learning rate\n",
        "* dL/dW = gradient of the loss function\n",
        "\n",
        "Example in Python:"
      ],
      "metadata": {
        "id": "V0a4DhnAjVVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "learning_rate = 0.01\n",
        "W = 2\n",
        "gradient = 5\n",
        "\n",
        "W = W - learning_rate * gradient\n",
        "print(W)"
      ],
      "metadata": {
        "id": "BBxIK1NwRmda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Pros: Works well for small datasets.\n",
        "* Cons: Computationally expensive for large datasets.\n",
        "\n",
        "**(b) Stochastic Gradient Descent**\n",
        "* Updates weights after each training example.\n",
        "* Used in online learning scenarios.\n",
        "\n",
        "Example in Python:"
      ],
      "metadata": {
        "id": "GbjTW2myRsOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import SGDRegressor\n",
        "\n",
        "model = SGDRegressor(learning_rate='constant', eta0=0.01)"
      ],
      "metadata": {
        "id": "y8ANQav8R8Uw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Pros: Faster updates, good for large datasets.\n",
        "* Cons: High variance in updates.\n",
        "\n",
        "**(c) Mini-Batch Gradient Descent**\n",
        "* A compromise between BGD and SGD.\n",
        "* Updates weights after a small batch of training examples.\n",
        "* Used in deep learning.\n",
        "\n",
        "Example in Python:"
      ],
      "metadata": {
        "id": "A9I9jOW2SLE6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "optimizer = SGD(learning_rate=0.01)"
      ],
      "metadata": {
        "id": "W675sViLS78z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Pros: Faster than BGD, more stable than SGD.\n",
        "* Cons: Requires careful tuning of batch size."
      ],
      "metadata": {
        "id": "GY2NM-fCTCih"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Non-Gradient-Based Optimizers**\n",
        "\n",
        "Some optimization techniques do not rely on gradient calculations:\n",
        "\n",
        "(g) Genetic Algorithms (GA)\n",
        "* Inspired by natural evolution.\n",
        "* Used for complex, non-differentiable problems.\n",
        "\n",
        "Example in Python:"
      ],
      "metadata": {
        "id": "p7wAa3XmUcKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from geneticalgorithm import geneticalgorithm as ga\n",
        "\n",
        "def f(x): return -x**2 + 4*x\n",
        "\n",
        "algorithm_param = {'max_num_iteration': 100}\n",
        "model = ga(function=f, dimension=1, variable_type='real', algorithm_parameters=algorithm_param)\n",
        "model.run()"
      ],
      "metadata": {
        "id": "zJeqwc4AUtBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Pros: Works well for non-convex problems.\n",
        "* Cons: Computationally expensive.\n",
        "\n",
        "**Summary: Which Optimizer to Use?**\n",
        "\n",
        "|Optimizer\t|Best Used For|\n",
        "|-||\n",
        "|SGD\t|Large-scale learning|\n",
        "|Momentum\t|Faster convergence, avoiding local minima|\n",
        "|RMSprop\t|Recurrent Neural Networks|\n",
        "|Adam\t|General-purpose deep learning models|\n",
        "|Genetic Algorithm\t|Optimization without gradients|"
      ],
      "metadata": {
        "id": "MWk0tLZPUwhZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 17. What is sklearn.linear_model ?\n",
        "**Ans** - sklearn.linear_model is a module in Scikit-Learn that provides linear models for regression and classification tasks. It includes algorithms like Linear Regression, Logistic Regression, Ridge, Lasso, and SGD-based models.\n",
        "\n",
        "**1. Linear Models for Regression**\n",
        "\n",
        "Regression models predict continuous values.\n",
        "\n",
        "(a) Linear Regression\n",
        "* Fits a straight line to the data using Ordinary Least Squares.\n",
        "\n",
        "Example: Linear Regression"
      ],
      "metadata": {
        "id": "4ezclMD7jWxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "\n",
        "X = np.array([[1], [2], [3], [4], [5]])\n",
        "y = np.array([2, 4, 6, 8, 10])\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "y_pred = model.predict([[6]])\n",
        "print(\"Prediction for X=6:\", y_pred[0])"
      ],
      "metadata": {
        "id": "1_yd-rStYFgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Pros: Simple, interpretable.\n",
        "* Cons: Assumes linear relationship.\n",
        "\n",
        "(b) Ridge Regression\n",
        "* Prevents overfitting by adding L2 penalty to the loss function.\n",
        "* Useful when features are highly correlated.\n",
        "\n",
        "Example: Ridge Regression"
      ],
      "metadata": {
        "id": "Nwtuz7mTYJo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "ridge = Ridge(alpha=1.0)\n",
        "ridge.fit(X, y)"
      ],
      "metadata": {
        "id": "bLuWgOqrYUJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(c) Lasso Regression\n",
        "* Prevents overfitting and performs feature selection by adding L1 penalty.\n",
        "* Shrinks some coefficients to zero, effectively removing irrelevant features.\n",
        "\n",
        "Example: Lasso Regression"
      ],
      "metadata": {
        "id": "bNq6YQfMYYNc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "lasso = Lasso(alpha=0.1)\n",
        "lasso.fit(X, y)"
      ],
      "metadata": {
        "id": "VWHgl8piYkfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Linear Models for Classification**\n",
        "\n",
        "Classification models predict discrete labels.\n",
        "\n",
        "(d) Logistic Regression\n",
        "* Used for binary or multiclass classification.\n",
        "\n",
        "Example: Logistic Regression"
      ],
      "metadata": {
        "id": "fXq_HDh5YnzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "X = np.array([[1], [2], [3], [4], [5]])\n",
        "y = np.array([0, 0, 1, 1, 1])\n",
        "\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(X, y)\n",
        "\n",
        "print(\"Prediction for X=2.5:\", log_reg.predict([[2.5]]))"
      ],
      "metadata": {
        "id": "w_4Z8KTSY4Mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Pros: Simple, efficient for classification.\n",
        "* Cons: Not ideal for complex, nonlinear data.\n",
        "\n",
        "(e) Stochastic Gradient Descent Classifier\n",
        "* Efficient for large-scale datasets.\n",
        "* Uses SGD to optimize models like Logistic Regression and SVMs.\n",
        "\n",
        "Example: SGD Classifier"
      ],
      "metadata": {
        "id": "J44doZ2fZAwh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "sgd_clf = SGDClassifier(loss='log_loss', max_iter=1000)\n",
        "sgd_clf.fit(X, y)"
      ],
      "metadata": {
        "id": "G_pdH1paZM8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Pros: Fast, works well with big data.\n",
        "* Cons: Sensitive to hyperparameters.\n",
        "\n",
        "**Summary**\n",
        "\n",
        "|Model\t|Use Case|\n",
        "|-||\n",
        "|Linear Regression\t|Predicting continuous values (e.g., house prices)|\n",
        "|Ridge Regression\t|Handling multicollinearity in regression|\n",
        "|Lasso Regression\t|Feature selection in regression|\n",
        "|Logistic Regression\t|Binary/multiclass classification|\n",
        "|SGD Classifier\t|Large-scale classification problems|"
      ],
      "metadata": {
        "id": "z1q0mOxmZPiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 18. What does model.fit() do? What arguments must be given?\n",
        "**Ans** - model.fit() is a method in Scikit-Learn that trains a machine learning model by learning patterns from the input data.\n",
        "* It adjusts model parameters based on the training data.\n",
        "* It minimizes the loss function to improve model accuracy.\n",
        "* It is used in both regression and classification tasks.\n",
        "\n",
        "**model.fit() Works Internally**\n",
        "1. Takes input features and target labels.\n",
        "2. Initializes model parameters.\n",
        "3. Applies the optimization algorithm to minimize loss.\n",
        "4. Iterates over training data until convergence or the maximum number of iterations is reached.\n",
        "\n",
        "Example: Training a Linear Regression Model"
      ],
      "metadata": {
        "id": "JrhFbEthkiUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "\n",
        "X = np.array([[1], [2], [3], [4], [5]])\n",
        "y = np.array([2, 4, 6, 8, 10])\n",
        "\n",
        "model = LinearRegression()\n",
        "\n",
        "model.fit(X, y)\n",
        "\n",
        "print(\"Intercept:\", model.intercept_)\n",
        "print(\"Coefficient:\", model.coef_)"
      ],
      "metadata": {
        "id": "0tjUAWmAaIes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The model learns that the coefficient is 2, meaning the equation y = 2x.\n",
        "\n",
        "**Arguments Required by fit()**\n",
        "\n",
        "model.fit(X, y, sample_weight=None)\n",
        "\n",
        "|Argument\t|Description|\n",
        "|-||\n",
        "|X\t|Input features|\n",
        "|y\t|Target labels|\n",
        "|sample_weight|Weights for each sample|\n",
        "\n",
        "**Example: Training a Logistic Regression Model**\n",
        "\n",
        "For classification problems, fit() learns how to separate data into classes."
      ],
      "metadata": {
        "id": "vHNPi0NZaMx3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "X = np.array([[1], [2], [3], [4], [5]])\n",
        "y = np.array([0, 0, 1, 1, 1])\n",
        "\n",
        "clf = LogisticRegression()\n",
        "clf.fit(X, y)\n",
        "\n",
        "print(\"Prediction for X=3:\", clf.predict([[3]]))"
      ],
      "metadata": {
        "id": "mBZlRUabasa5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The model classifies X=3 based on learned probabilities.\n",
        "\n",
        "**What Happens If You Don’t Call fit()?**\n",
        "\n",
        "If you try to use model.predict() without training (fit()), it will throw an error:\n",
        "* Error: NotFittedError: This LinearRegression instance is not fitted yet.\n",
        "\n",
        "**Summary**\n",
        "\n",
        "|Task\t|What fit() Does|\n",
        "|-||\n",
        "|Regression (Linear, Ridge, Lasso, etc.)\t|Learns best-fit line by minimizing error|\n",
        "|Classification (Logistic Regression, SVM, etc.)\t|Learns decision boundaries to separate classes|\n",
        "|Neural Networks (MLP, CNN, etc.)\t|Updates weights using backpropagation|"
      ],
      "metadata": {
        "id": "UoebsyOqa0IG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 19. What does model.predict() do? What arguments must be given?\n",
        "**Ans** - model.predict() is a method in Scikit-Learn that makes predictions using a trained machine learning model. It takes input features (X) and outputs predicted values (y^).\n",
        "\n",
        "* For Regression Models - Predicts a continuous value.\n",
        "* For Classification Models - Predicts class labels.\n",
        "\n",
        "**predict() Works Internally**\n",
        "1. Takes input features (X).\n",
        "2. Uses learned model parameters.\n",
        "3. Applies the model’s mathematical function.\n",
        "4. Outputs predictions (y^).\n",
        "\n",
        "**Example: predict() in Linear Regression**"
      ],
      "metadata": {
        "id": "yPQXG55Eklg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "\n",
        "X_train = np.array([[1], [2], [3], [4], [5]])\n",
        "y_train = np.array([2, 4, 6, 8, 10])\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "X_test = np.array([[6], [7]])\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Predictions:\", y_pred)"
      ],
      "metadata": {
        "id": "QeRdUHHOb7K2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Output: [12, 14] (since y = 2x).\n",
        "\n",
        "**Example: predict() in Classification (Logistic Regression)**\n",
        "\n",
        "For classification tasks, predict() returns class labels (0, 1, etc.)."
      ],
      "metadata": {
        "id": "cb8W7-hCb_C_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "X_train = np.array([[1], [2], [3], [4], [5]])\n",
        "y_train = np.array([0, 0, 1, 1, 1])\n",
        "\n",
        "clf = LogisticRegression()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "X_test = np.array([[2.5], [3.5]])\n",
        "predictions = clf.predict(X_test)\n",
        "\n",
        "print(\"Predicted Class Labels:\", predictions)"
      ],
      "metadata": {
        "id": "6zFLzaANcNRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Output: [0, 1] - The model predicts class 0 for X = 2.5 and class 1 for X = 3.5.\n",
        "\n",
        "**Arguments Required by predict()**\n",
        "model.predict(X)\n",
        "\n",
        "|Argument\t|Description|\n",
        "|-||\n",
        "|X\t|Input features (same format as training data)|\n",
        "\n",
        "* Important: X must have the same number of features as the model was trained on.\n",
        "\n",
        "**What If You Call predict() Before fit()?**"
      ],
      "metadata": {
        "id": "5sjy2rXJcQfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LinearRegression()\n",
        "model.predict([[6]])"
      ],
      "metadata": {
        "id": "K1QNxZu5cmQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Error: NotFittedError: This LinearRegression instance is not fitted yet.\n",
        "* Solution: Always train the model using fit() before calling predict()."
      ],
      "metadata": {
        "id": "YHy1Xg8hcokE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 20. What are continuous and categorical variables?\n",
        "**Ans** - ***Continuous vs. Categorical Variables**\n",
        "\n",
        "In data science and machine learning, variables are classified based on the type of data they represent. The two main types are continuous and categorical variables.\n",
        "\n",
        "**1. Continuous Variables**\n",
        "\n",
        "A continuous variable is a variable that can take any numerical value within a given range. These values are measured rather than counted and can have decimal points.\n",
        "* Characteristics of Continuous Variables:\n",
        "  * Can take infinitely many values within a range.\n",
        "  * Can have decimal points.\n",
        "  * Often measured rather than counted.\n",
        "\n",
        "**Examples:**\n",
        "* Height (e.g., 5.8 ft, 175.3cm)\n",
        "* Weight (e.g., 65.5kg, 150.2lbs)\n",
        "* Temperature (e.g., 36.6°C, 98.4°F)\n",
        "* Time (e.g., 2.45sec, 5.67hrs)\n",
        "* Price of a Product (e.g., $10.99, $99.50)\n",
        "\n",
        "**Types of Continuous Variables:**\n",
        "1. Interval Variable - Has no true zero (e.g., temperature in Celsius/Fahrenheit).\n",
        "2. Ratio Variable - Has a true zero (e.g., weight, height, distance).\n",
        "\n",
        "**Categorical Variables**\n",
        "\n",
        "A categorical variable is a variable that represents categories or groups rather than numerical values. These values are counted, not measured.\n",
        "\n",
        "**Characteristics of Categorical Variables:**\n",
        "* Represent distinct groups or labels.\n",
        "* Do not have numerical meaning.\n",
        "* Can be nominal or ordinal.\n",
        "\n",
        "**Examples:**\n",
        "* Gender (Male, Female, Other)\n",
        "* Blood Type (A, B, AB, O)\n",
        "* Color (Red, Blue, Green)\n",
        "* Education Level (High School, Bachelor's, Master's, PhD)\n",
        "* Marital Status (Single, Married, Divorced)\n",
        "\n",
        "**Types of Categorical Variables:**\n",
        "1. Nominal Variables - No meaningful order (e.g., eye color, car brand).\n",
        "2. Ordinal Variables - Have a meaningful order but no fixed difference (e.g., education levels, satisfaction ratings: Low, Medium, High).\n",
        "\n",
        "**3. Differences Between Continuous and Categorical Variables**\n",
        "\n",
        "|Feature\t|Continuous Variable\t|Categorical Variable|\n",
        "|-|||\n",
        "|Definition\t|Measured values that can take any number in a range\t|Represents groups or categories|\n",
        "|Numerical?\t|Yes\t|No|\n",
        "|Decimals?\t|Yes\t|No|\n",
        "|Examples\t|Height, Weight, Temperature\t|Gender, Car Brand, Blood Type|\n",
        "|Subtypes\t|Interval, Ratio\t|Nominal, Ordinal|\n",
        "\n",
        "**4. Identifying Continuous vs. Categorical Variables in Python**\n",
        "\n",
        "Example using Pandas"
      ],
      "metadata": {
        "id": "IAZGjlVKkngx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.DataFrame({\n",
        "    'Height': [170, 165, 180, 175],\n",
        "    'Weight': [65.5, 70.2, 80.1, 75.8],\n",
        "    'Gender': ['Male', 'Female', 'Male', 'Female'],\n",
        "    'Education': ['Bachelor', 'Master', 'PhD', 'High School']\n",
        "})\n",
        "\n",
        "categorical_cols = data.select_dtypes(include=['object']).columns\n",
        "continuous_cols = data.select_dtypes(include=['float64', 'int64']).columns\n",
        "\n",
        "print(\"Categorical Variables:\", categorical_cols.tolist())\n",
        "print(\"Continuous Variables:\", continuous_cols.tolist())"
      ],
      "metadata": {
        "id": "kikrBzT8eqXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 21. What is feature scaling? How does it help in Machine Learning?\n",
        "**Ans** - Feature scaling is a data preprocessing technique used to normalize or standardize numerical features so they have a consistent scale. This ensures that all features contribute equally to a machine learning model, preventing dominance by features with larger ranges.\n",
        "\n",
        "**Why is Feature Scaling Important?**\n",
        "* Improves Model Performance - Ensures that features with larger values do not overshadow smaller ones.\n",
        "* Accelerates Model Convergence - Helps optimization algorithms faster.\n",
        "* Prevents Numerical Instability - Avoids large variations in weight updates during training.\n",
        "* Required for Distance-Based Algorithms - Essential for models like KNN, SVM, PCA, K-Means, etc.\n",
        "\n",
        "**Types of Feature Scaling**\n",
        "\n",
        "There are two main types of feature scaling:\n",
        "\n",
        "**1. Min-Max Scaling**\n",
        "* Scales values between 0 and 1.\n",
        "* Retains the original distribution of data.\n",
        "* Best for deep learning and neural networks.\n",
        "\n",
        "Example in Python:"
      ],
      "metadata": {
        "id": "WQnT7kbBkpYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "data = np.array([[50], [100], [150], [200]])\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "print(scaled_data)"
      ],
      "metadata": {
        "id": "0FiMf97WfRwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Pros: Maintains the original data distribution.\n",
        "* Cons: Sensitive to outliers.\n",
        "\n",
        "**2. Standardization (Z-Score Scaling)**\n",
        "* Scales data to have mean = 0 and standard deviation = 1.\n",
        "* Best for models that assume normal distribution (e.g., Linear Regression, Logistic Regression, SVM).\n",
        "\n",
        "Example in Python:"
      ],
      "metadata": {
        "id": "P1hmkTrDfWGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "print(scaled_data)"
      ],
      "metadata": {
        "id": "JfM2OcqMfk4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Pros: Works well when data follows a normal distribution.\n",
        "* Cons: Transformed values are not in a fixed range.\n",
        "\n",
        "**3. Robust Scaling (Handles Outliers)**\n",
        "* Uses median and interquartile range (IQR) instead of mean and standard deviation.\n",
        "* Less sensitive to outliers compared to Min-Max and Standardization.\n",
        "\n",
        "Example in Python:"
      ],
      "metadata": {
        "id": "8NKxScETfosa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "scaler = RobustScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "print(scaled_data)"
      ],
      "metadata": {
        "id": "oICPTKMAf24F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Pros: Works well when data has outliers.\n",
        "* Cons: Doesn't perform well when data is normally distributed.\n",
        "\n",
        "**Choosing the Right Scaling Method**\n",
        "\n",
        "|Scaling Method\t|Use Case|\n",
        "|-||\n",
        "|Min-Max Scaling\t|When the data distribution is not normal (Neural Networks, KNN)|\n",
        "|Standardization (Z-Score Scaling)\t|When the data follows a normal distribution (Regression, SVM)|\n",
        "|Robust Scaling\t|When the data contains outliers|"
      ],
      "metadata": {
        "id": "OILnkJvTf5u0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 22. How do we perform scaling in Python?\n",
        "**Ans** - Feature scaling can be done using Scikit-Learn’s preprocessing module, which provides different scaling techniques such as Min-Max Scaling, Standardization, and Robust Scaling.\n",
        "\n",
        "**1. Min-Max Scaling (Normalization)**\n",
        "* Scales features between 0 and 1 (or any custom range).\n",
        "* Best for Neural Networks, KNN, Decision Trees.\n",
        "\n",
        "Example:"
      ],
      "metadata": {
        "id": "BxLtBJA9krkk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "data = np.array([[50], [100], [150], [200]])\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "print(\"Min-Max Scaled Data:\\n\", scaled_data)"
      ],
      "metadata": {
        "id": "fzVDQpJRgl3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Pros: Preserves original distribution.\n",
        "* Cons: Sensitive to outliers (small changes in min/max values can affect scaling).\n",
        "\n",
        "**2. Standardization (Z-Score Scaling)**\n",
        "* Scales data to have mean = 0 and standard deviation = 1.\n",
        "* Best for Linear Regression, Logistic Regression, SVM.\n",
        "\n",
        "Example:"
      ],
      "metadata": {
        "id": "qC0RmynAgqSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "print(\"Standard Scaled Data:\\n\", scaled_data)"
      ],
      "metadata": {
        "id": "PJSZ_pN6g6wp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Pros: Works well when data follows a normal distribution.\n",
        "* Cons: Does not bound values between 0 and 1.\n",
        "\n",
        "**3. Robust Scaling (Handles Outliers)**\n",
        "* Uses median and interquartile range (IQR) instead of mean & standard deviation.\n",
        "* Best when data contains outliers.\n",
        "\n",
        "Example:"
      ],
      "metadata": {
        "id": "p2dHv3N6g-DD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "scaler = RobustScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "print(\"Robust Scaled Data:\\n\", scaled_data)"
      ],
      "metadata": {
        "id": "E0Zxdx14hK2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Pros: Less sensitive to outliers.\n",
        "* Cons: Does not preserve exact distribution shape.\n",
        "\n",
        "**4. Scaling a Full Dataset (Multiple Features)**\n",
        "\n",
        "Example: Scaling Multiple Columns in Pandas DataFrame"
      ],
      "metadata": {
        "id": "7AbvVSoJhN97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'Age': [25, 30, 35, 40, 45],\n",
        "    'Salary': [50000, 60000, 70000, 80000, 90000]\n",
        "})\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
        "\n",
        "print(df_scaled)"
      ],
      "metadata": {
        "id": "GC3rCdlnhdtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* All numerical columns are scaled at once.\n",
        "\n",
        "**5. When to Use Different Scaling Methods?**\n",
        "\n",
        "|Scaling Method\t|Best Used For|\n",
        "|-||\n",
        "|Min-Max Scaling\t|Deep Learning, KNN, Decision Trees|\n",
        "|Standardization (Z-Score)\t|Regression, SVM, PCA\n",
        "Robust Scaling\t|Handling Outliers|"
      ],
      "metadata": {
        "id": "qFrfRfVJhhFH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 23. What is sklearn.preprocessing?\n",
        "**Ans** - sklearn.preprocessing is a module in Scikit-Learn that provides tools for scaling, normalizing, encoding, and transforming data before feeding it into a machine learning model. It helps in improving model performance by making features suitable for training.\n",
        "\n",
        "**Why Use sklearn.preprocessing?**\n",
        "* Ensures numerical features are on the same scale (important for models like SVM, KNN, and Gradient Descent-based algorithms).\n",
        "* Encodes categorical variables so they can be used in models.\n",
        "* Handles missing values and transforms skewed data.\n",
        "\n",
        "**Key Functions in sklearn.preprocessing**\n",
        "\n",
        "**1. Feature Scaling & Normalization**\n",
        "\n",
        "Scaling helps ensure all numerical features contribute equally.\n",
        "\n",
        "(a) Min-Max Scaling (Normalization)\n",
        "* Scales values between 0 and 1.\n",
        "* Best for Neural Networks, KNN, Decision Trees.\n",
        "\n",
        "Example:"
      ],
      "metadata": {
        "id": "TTSD1OZ9kuXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform([[50], [100], [150], [200]])\n",
        "print(scaled_data)"
      ],
      "metadata": {
        "id": "uhLnra6FnXie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) Standardization (Z-Score Scaling)\n",
        "* Scales data to have mean = 0, standard deviation = 1.\n",
        "* Best for Regression, SVM, PCA.\n",
        "\n",
        "Example:"
      ],
      "metadata": {
        "id": "pvlDtISsnarI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform([[50], [100], [150], [200]])\n",
        "print(scaled_data)"
      ],
      "metadata": {
        "id": "VczOg4Lqnkqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(c) Robust Scaling (Handles Outliers)\n",
        "* Uses median and interquartile range (IQR) instead of mean.\n",
        "* Best when data contains outliers.\n",
        "\n",
        "Example:"
      ],
      "metadata": {
        "id": "Pyowdkhonns7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "scaler = RobustScaler()\n",
        "scaled_data = scaler.fit_transform([[50], [100], [150], [200]])\n",
        "print(scaled_data)"
      ],
      "metadata": {
        "id": "-1QYxrHQnvTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Encoding Categorical Variables**\n",
        "\n",
        "Categorical features need to be converted into numerical values.\n",
        "\n",
        "(a) One-Hot Encoding (OHE)\n",
        "* Converts categories into binary columns.\n",
        "* Used when categories have no order (Nominal).\n",
        "\n",
        "Example:"
      ],
      "metadata": {
        "id": "8nnmzTefnyMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({'Color': ['Red', 'Blue', 'Green']})\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "encoded = encoder.fit_transform(df[['Color']])\n",
        "print(encoded)"
      ],
      "metadata": {
        "id": "-J4IT7-nn-LR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) Label Encoding\n",
        "* Assigns a unique integer to each category.\n",
        "* Best when categories have an order (Ordinal).\n",
        "\n",
        "Example:"
      ],
      "metadata": {
        "id": "_3vsSiIToBGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "encoded = encoder.fit_transform(['High', 'Medium', 'Low', 'High'])\n",
        "print(encoded)"
      ],
      "metadata": {
        "id": "RvH37TFqoIzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Handling Missing Values**\n",
        "* Fills missing values using mean, median, or mode.\n",
        "\n",
        "Example:"
      ],
      "metadata": {
        "id": "bCFsoemzoLW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "import numpy as np\n",
        "\n",
        "data = np.array([[1, 2], [np.nan, 3], [7, 6]])\n",
        "imputer = SimpleImputer(strategy=\"mean\")\n",
        "imputed_data = imputer.fit_transform(data)\n",
        "print(imputed_data)"
      ],
      "metadata": {
        "id": "MWq4kuQ8oTvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary of sklearn.preprocessing Functions**\n",
        "\n",
        "|Function\t|Purpose|\n",
        "|-||\n",
        "|MinMaxScaler()\t|Normalize data between 0 and 1|\n",
        "|StandardScaler()\t|Standardize data (mean=0, std=1)|\n",
        "|RobustScaler()\t|Scale using median (handles outliers)|\n",
        "|OneHotEncoder()\t|Convert categorical variables into binary columns|\n",
        "|LabelEncoder()\t|Convert categories into numerical values|\n",
        "|SimpleImputer()\t|Fill missing values using mean, median, etc.|"
      ],
      "metadata": {
        "id": "EP9U4cHnoWjd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 24. How do we split data for model fitting (training and testing) in Python?\n",
        "**Ans** - In machine learning, we split data into two (or three) sets:\n",
        "* Training Set – Used to train the model (70-80% of data).\n",
        "* Test Set – Used to evaluate the model’s performance (20-30% of data).\n",
        "* (Optional) Validation Set – Used for hyperparameter tuning (10-20% of data).\n",
        "\n",
        "**1. Splitting Data Using train_test_split()**\n",
        "\n",
        "The train_test_split() function from sklearn.model_selection is used to randomly divide data into training and testing sets.\n",
        "\n",
        "Example: Train-Test Split (80%-20%)"
      ],
      "metadata": {
        "id": "sJ3hIA6akv5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'Feature1': [10, 20, 30, 40, 50, 60, 70, 80],\n",
        "    'Feature2': [5, 15, 25, 35, 45, 55, 65, 75],\n",
        "    'Label': [1, 0, 1, 0, 1, 0, 1, 0]\n",
        "})\n",
        "\n",
        "train_set, test_set = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Training Set:\\n\", train_set)\n",
        "print(\"\\nTesting Set:\\n\", test_set)"
      ],
      "metadata": {
        "id": "nKF-tpMlps0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* test_size=0.2 → 20% of data is reserved for testing.\n",
        "* random_state=42 → Ensures reproducibility.\n",
        "\n",
        "**2. Splitting Features and Labels (X and y)**\n",
        "\n",
        "Example: Splitting X (features) and y (target labels)"
      ],
      "metadata": {
        "id": "x5Rt96Yxpxj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop(columns=['Label'])\n",
        "y = df['Label']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"X_train:\\n\", X_train)\n",
        "print(\"y_train:\\n\", y_train)"
      ],
      "metadata": {
        "id": "vXDA8GrCqDKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The model will train on X_train, y_train and be evaluated on X_test, y_test.\n",
        "\n",
        "**3. Splitting Data Into Training, Validation, and Test Sets**\n",
        "\n",
        "If we need a validation set for hyperparameter tuning, we can split the data further.\n",
        "\n",
        "Example: 60% Train, 20% Validation, 20% Test"
      ],
      "metadata": {
        "id": "LrDg7sX4qGwN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
        "\n",
        "print(\"Training Set Size:\", len(X_train))\n",
        "print(\"Validation Set Size:\", len(X_val))\n",
        "print(\"Testing Set Size:\", len(X_test))"
      ],
      "metadata": {
        "id": "1iqLb9q4qXP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* This results in:\n",
        "  * 60% Training\n",
        "  * 20% Validation\n",
        "  * 20% Testing\n",
        "\n",
        "**4. Stratified Sampling for Imbalanced Datasets**\n",
        "\n",
        "If the dataset is imbalanced (e.g., fraud detection), use stratify=y to maintain the same class proportions.\n",
        "\n",
        "Example: Stratified Split"
      ],
      "metadata": {
        "id": "BrguwtHmqaYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
      ],
      "metadata": {
        "id": "yZkT8PHJqqe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Ensures equal class distribution in training and test sets.\n",
        "\n",
        "**5. When to Use Different Splits?**\n",
        "\n",
        "|Scenario\t|Train-Test Split|\n",
        "|-||\n",
        "|Small Dataset (< 1,000 samples)\t|80% Train, 20% Test|\n",
        "|Large Dataset (> 10,000 samples)\t|70% Train, 20% Test, 10% Validation|\n",
        "|Deep Learning\t|60% Train, 20% Validation, 20% Test|\n",
        "|Imbalanced Data\t|Use stratify=y|"
      ],
      "metadata": {
        "id": "-hGbjUN6qtvr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 25. Explain data encoding\n",
        "**Ans** - Data encoding is the process of converting categorical variables into numerical format so that machine learning models can process them. Since most ML algorithms work with numerical data, categorical features need to be transformed appropriately.\n",
        "\n",
        "**Why is Data Encoding Important?**\n",
        "* Machine Learning models require numerical inputs.\n",
        "* Categorical features need to be converted into a usable form.\n",
        "* Ensures the model understands relationships between categories.\n",
        "\n",
        "**Types of Data Encoding**\n",
        "\n",
        "**1. Label Encoding (Ordinal Encoding)**\n",
        "* Assigns a unique integer to each category.\n",
        "* Suitable for ordinal data (categories with a meaningful order).\n",
        "\n",
        "Example:\n",
        "\n",
        "|Education Level\t|Encoded Value|\n",
        "|-||\n",
        "|High School\t|0|\n",
        "|Bachelor's\t|1|\n",
        "|Master's\t|2|\n",
        "|PhD\t|3|\n",
        "\n",
        "Python Implementation:"
      ],
      "metadata": {
        "id": "1ypkyoIckxaj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "data = ['High School', 'Bachelor', 'Master', 'PhD']\n",
        "encoder = LabelEncoder()\n",
        "encoded_data = encoder.fit_transform(data)\n",
        "\n",
        "print(encoded_data)"
      ],
      "metadata": {
        "id": "F-PmZgh5uupY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Pros: Simple and memory-efficient.\n",
        "* Cons: Models might assume numerical relationships where none exist.\n",
        "\n",
        "**2. One-Hot Encoding (OHE)**\n",
        "* Converts categories into binary columns (0s and 1s).\n",
        "* Suitable for nominal data (categories with no order).\n",
        "\n",
        "Example:\n",
        "\n",
        "|Color\t|Red\t|Blue\t|Green|\n",
        "|-||||\n",
        "|Red\t|1\t|0\t|0|\n",
        "|Blue\t|0\t|1\t|0|\n",
        "|Green\t|0\t|0\t|1|\n",
        "\n",
        "Python Implementation:"
      ],
      "metadata": {
        "id": "eUzqdbyTu0Zs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({'Color': ['Red', 'Blue', 'Green']})\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "encoded = encoder.fit_transform(df[['Color']])\n",
        "\n",
        "print(encoded)"
      ],
      "metadata": {
        "id": "cBYnS4vPvd8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Pros: No assumption of numerical order.\n",
        "* Cons: Can create too many columns for high-cardinality features.\n",
        "\n",
        "**3. Ordinal Encoding**\n",
        "* Similar to Label Encoding, but we specify a meaningful order.\n",
        "\n",
        "Example:\n",
        "\n",
        "|Size\t|Encoded Value|\n",
        "|-||\n",
        "|Small\t|0|\n",
        "|Medium\t|1|\n",
        "|Large\t|2|\n",
        "\n",
        "Python Implementation:"
      ],
      "metadata": {
        "id": "oKFMbIokvhyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "df = pd.DataFrame({'Size': ['Small', 'Medium', 'Large']})\n",
        "encoder = OrdinalEncoder(categories=[['Small', 'Medium', 'Large']])\n",
        "df['Size_encoded'] = encoder.fit_transform(df[['Size']])\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "id": "zFI6X3kiv2mv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Pros: Preserves ordinal relationships.\n",
        "* Cons: Should only be used when order matters.\n",
        "\n",
        "**4. Target Encoding (Mean Encoding)**\n",
        "* Replaces categories with the mean of the target variable.\n",
        "* Useful for high-cardinality categorical features.\n",
        "\n",
        "Example: Predicting house prices\n",
        "\n",
        "|Neighborhood\t|Average Price ($1000s)|\n",
        "|-||\n",
        "|A\t|200|\n",
        "|B\t|250|\n",
        "|C\t|300|\n",
        "\n",
        "Python Implementation:"
      ],
      "metadata": {
        "id": "pabxb1KBwGF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({'Neighborhood': ['A', 'B', 'A', 'C'], 'Price': [200, 250, 220, 300]})\n",
        "df['Neighborhood_encoded'] = df.groupby('Neighborhood')['Price'].transform('mean')\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "id": "j3LAZtJ1wahL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Pros: Reduces dimensionality.\n",
        "* Cons: Can lead to data leakage if applied incorrectly.\n",
        "\n",
        "**5. Frequency (Count) Encoding**\n",
        "* Replaces categories with their occurrence count in the dataset.\n",
        "* Useful when some categories appear much more frequently.\n",
        "\n",
        "Example:\n",
        "\n",
        "|City\t|Count|\n",
        "|-||\n",
        "|New York\t|2|\n",
        "|Los Angeles\t|1|\n",
        "|Chicago\t|2|\n",
        "\n",
        "Python Implementation:"
      ],
      "metadata": {
        "id": "bzsZ5Fw1wddv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['City_encoded'] = df['City'].map(df['City'].value_counts())"
      ],
      "metadata": {
        "id": "7Ttq-tqGwxb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Pros: Keeps useful information about category importance.\n",
        "* Cons: May not always capture meaningful patterns.\n",
        "\n",
        "**Choosing the Right Encoding Technique**\n",
        "\n",
        "|Encoding Type\t|Use Case|\n",
        "|-||\n",
        "|Label Encoding\t|Ordinal categories (e.g., Education Level)|\n",
        "|One-Hot Encoding\t|Small number of categories (e.g., Colors)|\n",
        "|Ordinal Encoding\t|When order matters (e.g., Small, Medium, Large)|\n",
        "|Target Encoding\t|High-cardinality categorical features (e.g., Zip Codes)|\n",
        "|Frequency Encoding\t|When frequency matters (e.g., Popularity of locations)|"
      ],
      "metadata": {
        "id": "0n-F-uTow0pK"
      }
    }
  ]
}